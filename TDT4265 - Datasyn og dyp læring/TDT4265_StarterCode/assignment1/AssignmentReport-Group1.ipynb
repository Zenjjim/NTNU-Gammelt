{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](task1_a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "![](task1_b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)\n",
    "\n",
    "Final Train Cross Entropy Loss: 0.06764842647882187\n",
    "\n",
    "Final Validation Cross Entropy Loss: 0.08103037334021336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)\n",
    "\n",
    "Train accuracy: 0.978194866133039\n",
    "\n",
    "Validation accuracy: 0.9788732394366197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "For me the early stopped kicked in after 10 epoch. The result was:\n",
    "\n",
    "Final Train Cross Entropy Loss: 0.08958487019189541\n",
    "\n",
    "Final Validation Cross Entropy Loss: 0.08835086674263219\n",
    "\n",
    "Train accuracy: 0.9688103781396633\n",
    "\n",
    "Validation accuracy: 0.9765258215962441"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "The rest of the tasks are done with the original 50 epoch and without early stopping:\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)\n",
    "\n",
    "We can see that shuffling the loss has a lot fewer spikes and a more controlled validation accuracy. This happens because a difficult batch of training data (data that is hard to classify) is not repeated for every epoch. This hard to classicy data can be seen every 200 steps as a huge nagative spike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "We can see that the training accuracy is outpacing the validation accuracy as the traning model gets better and better at classifying on the small data set but it performs worse on the data set that it is not trained on (validation set). Therefor yes we can see sign off overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "![](task4_a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "![](task4b_softmax_weight.png)\n",
    "\n",
    "We can see that the lower one (lambda = 2) has a lot less noise. The reason for this is because the dR is controlling that the weight is not freely incresing. So the effect of increasing the weight has to be greater then the negative effect of increasing the weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "\n",
    "L2 is used to help prevent overfitting in complex problems with a small dataset. But since we have a fairly simple problem and a LOT of data this is not really and issue. Instead of preventing overfitting L2 is preventing the model from learning instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "![](task4e_l2_reg_norms.png)\n",
    "\n",
    "As the lambda is increasing the L2 norm is decreasing. This is not surprising as we have stated earlier that cost function penalizes the magnitude of w. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6e7c92f01f6b07e1f588851d8f416c19a631ef52666baed76dc6589d64e45"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
